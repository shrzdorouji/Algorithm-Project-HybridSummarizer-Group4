"""
LLM-Based Abstractive Summarization Module
------------------------------------------
Model: Pegasus (Distilled) - Fully Offline Version
"""

from typing import Optional
import re
import os


class LLMAbstractiveSummarizer:
    def __init__(
            self,
            # Ø¢Ø¯Ø±Ø³ Ù¾ÙˆØ´Ù‡â€ŒØ§ÛŒ Ú©Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ Ø¯Ø± Ø¢Ù† Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯
            model_path: str = "./my_pegasus",
            max_length: int = 150,
            prompt_template: Optional[str] = None,
    ):
        self.max_length = max_length
        self.prompt_template = prompt_template or "{document}"

        try:
            # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ù…Ø³ØªÙ‚ÛŒÙ… Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØµÙˆØµ Ù¾Ú¯Ø§Ø³ÙˆØ³ Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø¨ÛŒØ´ØªØ±
            from transformers import pipeline, PegasusTokenizer, PegasusForConditionalGeneration

            print(f"ðŸ”„ Loading Pegasus from local directory: {model_path}")

            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ù…Ø¯Ù„ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø§Ø² Ù¾ÙˆØ´Ù‡ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ø´Ù…Ø§
            tokenizer = PegasusTokenizer.from_pretrained(model_path)
            model = PegasusForConditionalGeneration.from_pretrained(model_path)

            # Ø§ÛŒØ¬Ø§Ø¯ Ø®Ø· Ù„ÙˆÙ„Ù‡ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø­Ù„ÛŒ
            self.summarizer = pipeline(
                "summarization",
                model=model,
                tokenizer=tokenizer,
                device=-1  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CPU Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¹Ø¯Ù… ØªØ¯Ø§Ø®Ù„ Ø¨Ø§ Ú©Ø§Ø±Øª Ú¯Ø±Ø§ÙÛŒÚ©
            )
            print("âœ… Pegasus Engine is fully loaded and ready!")

        except Exception as e:
            print(f"âŒ Error loading local model: {e}")
            print("ðŸ’¡ Tip: Ensure all 5 files (including pytorch_model.bin) are in 'my_pegasus' folder.")

    def preprocess(self, document: str) -> str:
        if not document or not document.strip():
            return ""
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙÙˆØ§ØµÙ„ Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ú© Ø¨Ù‡ØªØ± Ù…Ø¯Ù„
        text = document.strip()
        text = re.sub(r'\s+', ' ', text)
        return text

    def build_prompt(self, document: str) -> str:
        """
        Construct the final LLM input by combining
        the fixed prompt and the document text.

        Parameters
        ----------
        document : str
            Preprocessed document.

        Returns
        -------
        str
            Final input prompt for the LLM.
        """
        pass

    # ------------------------------------------------------------------
    # Step 3: LLM Generation
    # ------------------------------------------------------------------
    def generate_summary(self, prompt: str) -> str:
        """
        Generate an abstractive summary using an external LLM.

        NOTE:
        - The LLM is treated as a black-box oracle.
        - No internal states, scores, or explanations are exposed.

        Parameters
        ----------
        prompt : str
            Input prompt for the LLM.

        Returns
        -------
        str
            Abstractive summary S_llm.
        """
        pass

    # ------------------------------------------------------------------
    # Main Interface
    # ------------------------------------------------------------------
    def summarize(self, document: str) -> str:
        """
        Generate a standalone abstractive summary from the original document.

        Algorithm:
        1. Preprocess document D
        2. Construct prompt I = P + D
        3. Generate summary S_llm = LLM.generate(I)
        4. Return S_llm

        Parameters
        ----------
        document : str
            Original input document.

        Returns
        -------
        str
            Abstractive LLM-generated summary.
        """
        processed_doc = self.preprocess(document)
        prompt = self.build_prompt(processed_doc)
        summary = self.generate_summary(prompt)
        return summary
