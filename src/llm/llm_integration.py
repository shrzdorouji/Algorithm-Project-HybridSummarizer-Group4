"""
LLM-Based Abstractive Summarization Module
------------------------------------------
Model: Pegasus (Distilled) - Fully Offline Version
"""

from typing import Optional
import re
import os


class LLMAbstractiveSummarizer:
    def __init__(
            self,
            # Ø¢Ø¯Ø±Ø³ Ù¾ÙˆØ´Ù‡â€ŒØ§ÛŒ Ú©Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ Ø¯Ø± Ø¢Ù† Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯
            model_path: str = "./my_pegasus",
            max_length: int = 150,
            prompt_template: Optional[str] = None,
    ):
        self.max_length = max_length
        self.prompt_template = prompt_template or "{document}"

        try:
            # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ù…Ø³ØªÙ‚ÛŒÙ… Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØµÙˆØµ Ù¾Ú¯Ø§Ø³ÙˆØ³ Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø¨ÛŒØ´ØªØ±
            from transformers import pipeline, PegasusTokenizer, PegasusForConditionalGeneration

            print(f"ğŸ”„ Loading Pegasus from local directory: {model_path}")

            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ù…Ø¯Ù„ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø§Ø² Ù¾ÙˆØ´Ù‡ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ø´Ù…Ø§
            tokenizer = PegasusTokenizer.from_pretrained(model_path)
            model = PegasusForConditionalGeneration.from_pretrained(model_path)

            # Ø§ÛŒØ¬Ø§Ø¯ Ø®Ø· Ù„ÙˆÙ„Ù‡ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø­Ù„ÛŒ
            self.summarizer = pipeline(
                "summarization",
                model=model,
                tokenizer=tokenizer,
                device=-1  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CPU Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¹Ø¯Ù… ØªØ¯Ø§Ø®Ù„ Ø¨Ø§ Ú©Ø§Ø±Øª Ú¯Ø±Ø§ÙÛŒÚ©
            )
            print("âœ… Pegasus Engine is fully loaded and ready!")

        except Exception as e:
            print(f"âŒ Error loading local model: {e}")
            print("ğŸ’¡ Tip: Ensure all 5 files (including pytorch_model.bin) are in 'my_pegasus' folder.")

    def preprocess(self, document: str) -> str:
        if not document or not document.strip():
            return ""
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙÙˆØ§ØµÙ„ Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ú© Ø¨Ù‡ØªØ± Ù…Ø¯Ù„
        text = document.strip()
        text = re.sub(r'\s+', ' ', text)
        return text

    def build_prompt(self, document: str) -> str:
        if not document:
            return ""
        return self.prompt_template.format(document=document)

    def generate_summary(self, prompt: str) -> str:
        if not prompt:
            return ""

        try:
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ù¾Ú¯Ø§Ø³ÙˆØ³ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ (Abstractive)
            outputs = self.summarizer(
                prompt,
                max_length=60,
                min_length=20,
                num_beams=4,  # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¹Ù…ÛŒÙ‚ Ú©Ù„Ù…Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ù†Ø³Ø§Ù†ÛŒ
                no_repeat_ngram_size=3,  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± Ø¹ÛŒÙ† Ø¹Ø¨Ø§Ø±Ø§Øª Ù…ØªÙ† Ø§ØµÙ„ÛŒ
                repetition_penalty=1.5,  # Ø¬Ø±ÛŒÙ…Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ú©Ù¾ÛŒâ€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ú©Ù„Ù…Ù‡ Ø¨Ù‡ Ú©Ù„Ù…Ù‡
                length_penalty=0.8,  # ØªØ¹Ø§Ø¯Ù„ Ø¨ÛŒÙ† Ú©ÙˆØªØ§Ù‡ÛŒ Ùˆ Ú©Ø§Ù…Ù„ Ø¨ÙˆØ¯Ù† Ù…ØªÙ†
                early_stopping=True,
                truncation=True
            )

            res = outputs[0]['summary_text'].strip()
            # Pegasus Ú¯Ø§Ù‡ÛŒ ØªÚ¯ <n> Ø¨Ø±Ø§ÛŒ Ø®Ø· Ø¬Ø¯ÛŒØ¯ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢Ù† Ø±Ø§ Ø¨Ø§ ÙØ§ØµÙ„Ù‡ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            return res.replace("<n>", " ").strip()

        except Exception as e:
            print(f"âš ï¸ LLM Generation Error: {e}")
            # Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ø­Ø§Ù„Øª Ø§Ù…Ù†: Ù†Ù…Ø§ÛŒØ´ Ø¨Ø®Ø´ÛŒ Ø§Ø² Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø±ÙˆØ² Ø®Ø·Ø§
            return " ".join(prompt.split()[:25]) + "..."

    def summarize(self, document: str) -> str:
        processed_doc = self.preprocess(document)
        input_text = self.build_prompt(processed_doc)
        return self.generate_summary(input_text)