"""
LLM-Based Abstractive Summarization Module
------------------------------------------
Model: Pegasus (Distilled) - Fully Offline Version
"""

from typing import Optional
import re
import os

from nltk.translate.lepor import length_penalty


class LLMAbstractiveSummarizer:
    def __init__(
            self,
            # Ø¢Ø¯Ø±Ø³ Ù¾ÙˆØ´Ù‡â€ŒØ§ÛŒ Ú©Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ Ø¯Ø± Ø¢Ù† Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯
            model_path: str = "./my_pegasus",
            max_length: int = 150,
            prompt_template: Optional[str] = None,
    ):
        self.max_length = max_length
        self.prompt_template = prompt_template or "{document}"

        try:
            # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ù…Ø³ØªÙ‚ÛŒÙ… Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØµÙˆØµ Ù¾Ú¯Ø§Ø³ÙˆØ³ Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø¨ÛŒØ´ØªØ±
            from transformers import pipeline, PegasusTokenizer, PegasusForConditionalGeneration

            print(f"ğŸ”„ Loading Pegasus from local directory: {model_path}")

            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ù…Ø¯Ù„ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø§Ø² Ù¾ÙˆØ´Ù‡ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ø´Ù…Ø§
            tokenizer = PegasusTokenizer.from_pretrained(model_path)
            model = PegasusForConditionalGeneration.from_pretrained(model_path)

            # Ø§ÛŒØ¬Ø§Ø¯ Ø®Ø· Ù„ÙˆÙ„Ù‡ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø­Ù„ÛŒ
            self.summarizer = pipeline(
                "summarization",
                model=model,
                tokenizer=tokenizer,
                device=-1  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CPU Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¹Ø¯Ù… ØªØ¯Ø§Ø®Ù„ Ø¨Ø§ Ú©Ø§Ø±Øª Ú¯Ø±Ø§ÙÛŒÚ©
            )
            print("âœ… Pegasus Engine is fully loaded and ready!")

        except Exception as e:
            print(f"âŒ Error loading local model: {e}")
            print("ğŸ’¡ Tip: Ensure all 5 files (including pytorch_model.bin) are in 'my_pegasus' folder.")

    def preprocess(self, document: str) -> str:
        if not document or not document.strip():
            return ""
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÙÙˆØ§ØµÙ„ Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ú© Ø¨Ù‡ØªØ± Ù…Ø¯Ù„
        text = document.strip()
        text = re.sub(r'\s+', ' ', text)
        return text

    def build_prompt(self, document: str) -> str:
        if not document:
            return ""
        return self.prompt_template.format(document=document)

    def generate_summary(self, prompt: str) -> str:
        if not prompt:
            return ""

        try:
            # ØªØºÛŒÛŒØ±Ø§Øª Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒÚ© Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡
            outputs = self.summarizer(
                prompt,
                max_length=60,
                min_length=30,

                # --- ØªØºÛŒÛŒØ±Ø§Øª Ø§ØµÙ„ÛŒ Ø§ÛŒÙ†Ø¬Ø§Ø³Øª ---
                do_sample=True,  # ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§Ù‚ÛŒØª Ø¨ÛŒØ´ØªØ±
                top_k=50,  # Ø§Ù†ØªØ®Ø§Ø¨ Ø§Ø² Ø¨ÛŒÙ† ÛµÛ° Ú©Ù„Ù…Ù‡ Ø¨Ø±ØªØ±
                top_p=0.95,  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÚ©Ù†ÛŒÚ© Nucleus Sampling
                temperature=1.2,  # Ú©Ù†ØªØ±Ù„ Ù…ÛŒØ²Ø§Ù† Ø®Ù„Ø§Ù‚ÛŒØª (Ø¹Ø¯Ø¯ Ø¨Ø§Ù„Ø§ØªØ± = Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø¨ÛŒØ´ØªØ±)

                no_repeat_ngram_size=2,  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± Ø¹Ø¨Ø§Ø±Ø§Øª Û³ Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ù…ØªÙ† Ø§ØµÙ„ÛŒ
                repetition_penalty=10.0,  # Ø¬Ø±ÛŒÙ…Ù‡ Ø³Ù†Ú¯ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª
                # --------------------------
                length_penalty=1.5,
                truncation=True
            )

            res = outputs[0]['summary_text'].strip()
            # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
            return res.replace("<n>", " ").strip()

        except Exception as e:
            print(f"âš ï¸ LLM Generation Error: {e}")
            return " ".join(prompt.split()[:25]) + "..."

    def summarize(self, document: str) -> str:
        processed_doc = self.preprocess(document)
        input_text = self.build_prompt(processed_doc)
        return self.generate_summary(input_text)