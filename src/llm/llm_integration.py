from typing import Optional
import re
import os

class LLMAbstractiveSummarizer:
    def __init__(
            self,
            model_path: str = "./my_pegasus",
            max_length: int = 150,
            prompt_template: Optional[str] = None,
    ):
        self.max_length = max_length
        self.prompt_template = prompt_template or "{document}"

        try:
            from transformers import pipeline, PegasusTokenizer, PegasusForConditionalGeneration

            print(f"ğŸ”„ Loading Pegasus from local directory: {model_path}")
            tokenizer = PegasusTokenizer.from_pretrained(model_path)
            model = PegasusForConditionalGeneration.from_pretrained(model_path)

            self.summarizer = pipeline(
                "summarization",
                model=model,
                tokenizer=tokenizer,
                device=-1  # CPU
            )
            print("âœ… Pegasus Engine is ready with advanced sampling!")

        except Exception as e:
            print(f"âŒ Error loading local model: {e}")

    def preprocess(self, document: str) -> str:
        if not document or not document.strip():
            return ""
        text = document.strip()
        text = re.sub(r'\s+', ' ', text)
        return text

    def build_prompt(self, document: str) -> str:
        if not document:
            return ""
        # Ø·Ø¨Ù‚ Ù†Ø¸Ø± Ø§Ø³ØªØ§Ø¯ Ø¨Ø±Ø§ÛŒ Ø²ÛŒØ±ÙˆØ´Ø§Øª Ø¨Ù‡ØªØ±ØŒ Ø¯Ø³ØªÙˆØ± ØµØ±ÛŒØ­â€ŒØªØ± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
        return f"Summarize and paraphrase the following: {document}"

    def generate_summary(self, prompt: str) -> str:
        if not prompt: return ""
        try:
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø·ÙˆÙ„ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ù‡ Ú©Ù„Ù…Ù‡
            input_token_len = len(prompt.split())

            # ØªØ¹ÛŒÛŒÙ† Ø³Ù‚Ù Ø®Ø±ÙˆØ¬ÛŒ: ÛŒØ§ 60% Ø·ÙˆÙ„ ÙˆØ±ÙˆØ¯ÛŒØŒ ÛŒØ§ Ø­Ø¯Ø§Ú©Ø«Ø± 80 Ú©Ù„Ù…Ù‡ (Ù‡Ø± Ú©Ø¯Ø§Ù… Ú©Ù…ØªØ± Ø¨ÙˆØ¯)
            dynamic_max = min(150, int(input_token_len * 0.6))
            dynamic_min = min(5, int(input_token_len * 0.5)) # Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡ØŒ Ø­Ø¯Ø§Ù‚Ù„ Ø±Ø§ Ø±ÙˆÛŒ Ûµ Ø¨Ú¯Ø°Ø§Ø±

            outputs = self.summarizer(
                prompt,
                max_length=dynamic_max,
                min_length=dynamic_min,
                do_sample=True,
                top_k=40,
                top_p=0.90,
                temperature=0.8,  # Ø¯Ù…Ø§ÛŒ Ù…ØªØ¹Ø§Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ ØªÙˆÙ‡Ù… (Hallucination)
                repetition_penalty=3.5,
                no_repeat_ngram_size=2,
                num_beams=1,
                length_penalty=1.0,  # Ø®Ù†Ø«ÛŒ Ú©Ø±Ø¯Ù† Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Warning
                early_stopping=False,  # Ø®Ù†Ø«ÛŒ Ú©Ø±Ø¯Ù† Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Warning
                truncation=True
            )

            res = outputs[0]['summary_text'].strip()
            # ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù¾Ú¯Ø§Ø³ÙˆØ³
            res = res.replace("<n>", " ").replace(" .", ".").strip()
            return res

        except Exception as e:
            print(f"âš ï¸ Generation Error: {e}")
            return " ".join(prompt.split()[:dynamic_max]) if 'dynamic_max' in locals() else ""

    def summarize(self, document: str) -> str:
        processed_doc = self.preprocess(document)
        input_text = self.build_prompt(processed_doc)
        return self.generate_summary(input_text)