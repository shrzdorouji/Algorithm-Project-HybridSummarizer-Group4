# LLM-Based Summarization Module

This document describes the role and behavior of the **Large Language Model (LLM)**
used in this project as an **independent abstractive summarization component**.

---

## 1. Definition of LLM

A Large Language Model (LLM) is a neural network trained on large-scale textual data
to model the probability distribution of natural language.
LLMs are capable of understanding semantic relationships and generating coherent text.

In this project, the LLM is used exclusively for **abstractive summarization**.

---

## 2. Role of the LLM in This Project

The LLM generates a **standalone summary** directly from the original document.
Its output is independent of the TextRank algorithm.

The overall pipeline is as follows:

1. Original document → TextRank summary
2. Original document → LLM-based summary
3. TextRank summary + LLM summary → Hybrid Merge Algorithm (proposed)

The LLM does not interact with TextRank and does not affect sentence ranking.

---

## 3. Input to the LLM

- Full original document
- Preprocessed text (tokenized and cleaned)
- No sentence scores or graph information are provided

The LLM receives only raw textual content.

---

## 4. Output of the LLM

- A fluent, coherent abstractive summary
- The summary may paraphrase or compress content
- Sentence boundaries are not required to match the original text

The output length is controlled using fixed parameters (e.g., maximum token limit).

---

## 5. Constraints and Limitations

Despite its semantic strength, the LLM has several limitations:

- Possible hallucination (generation of information not present in the text)
- Lack of explicit explainability
- Higher computational cost compared to TextRank
- Non-deterministic outputs

For these reasons, the LLM output is **not considered final** and must be validated
through a hybrid merging algorithm.

---

## 6. Summary

In this project, the LLM serves as an **independent abstractive summarizer**.
Its output complements the extractive summary produced by TextRank.
The final summary is generated by a separate hybrid algorithm that combines
the strengths of both approaches.
## 7. Optional Pseudocode for LLM Usage

The following pseudocode illustrates how the LLM is used as an external
abstractive summarization module.
```text
Algorithm: LLM_Abstractive_Summarization

Input:
D : Original document
L : Maximum summary length
P : Fixed summarization prompt

Output:
S_llm : Abstractive summary

Steps:
1. Preprocess document D
2. Construct input I = concatenate(P, D)
3. Generate summary S_llm = LLM.generate(I, max_length = L)
4. Return S_llm

