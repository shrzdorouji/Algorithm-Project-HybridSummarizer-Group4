# Hybrid Merge Algorithm for Summarization

This document presents the **Hybrid Merge Algorithm**, a controlled integration method
for combining an extractive summary generated by **TextRank** and an abstractive summary
generated by a **Large Language Model (LLM)** into a single, coherent final summary.

The LLM is treated as an auxiliary semantic module, not a decision-maker.
All final selections are governed by an explicit algorithmic process.

---

## 1. Purpose

The Hybrid Merge Algorithm performs **four core tasks**:

1. **Merge**  
   Combine candidate sentences from TextRank and the LLM.

2. **Compare**  
   Evaluate importance and semantic redundancy among candidates.

3. **Rank**  
   Assign a combined score using weighted TextRank and LLM contributions.

4. **Generate**  
   Select a limited number of non-redundant sentences to form the final summary.

This design ensures that the final output preserves **structural importance**
(from TextRank) while benefiting from **semantic richness and fluency**
introduced by the LLM.

---

## 2. Inputs and Outputs

### Inputs

- `S_textrank` : List of top-k extractive sentences selected by TextRank.  
- `S_llm` : Abstractive summary sentences generated by the LLM.  
- `L_max` : Maximum number of sentences allowed in the final summary.  
- `alpha`, `beta` : Weight parameters for TextRank and LLM scores (`alpha + beta = 1`).  
  Recommended default: `alpha = 0.6`, `beta = 0.4`.  
- `sim_threshold` : Redundancy threshold based on cosine similarity (e.g., 0.7).  

### Output

- `S_hybrid` : Final hybrid summary (ordered list of sentences).

---

## 3. Algorithm Overview

### Step 1: Merge
Collect all candidate sentences from both sources without early filtering.

### Step 2: Compare and Score
Each sentence receives:

- a **TextRank score** if it originates from `S_textrank`,  
- an **LLM semantic score** if it originates from `S_llm`.

LLM contribution is measured as **semantic similarity to the original document** using sentence embeddings (e.g., BERT, Sentence-BERT, or Universal Sentence Encoder).

### Step 3: Rank
All candidate sentences are globally ranked according to a **weighted combined score**:

\[
\text{Score}(s_i) = \alpha \cdot \text{TR\_Score}(s_i) + \beta \cdot \text{LLM\_Score}(s_i)
\]

where:  

- `TR_Score(s_i)` reflects structural importance (TextRank),  
- `LLM_Score(s_i)` reflects semantic relevance (LLM),  
- `α, β` control the balance between extractive and abstractive signals.

### Step 4: Generate Summary
Iteratively select sentences from the ranked list while:

- Respecting the length limit `L_max`.  
- Preventing semantic redundancy: exclude any sentence whose cosine similarity with **any already selected sentence** exceeds `sim_threshold`.  

Optional postprocessing can reorder sentences and apply minor paraphrasing to improve fluency.

---

## 4. Pseudocode

```
Algorithm: Hybrid_Merge_Summarization

Input:
    S_textrank      : list of top-k sentences from TextRank
    S_llm           : abstractive summary sentences from LLM
    L_max           : maximum number of sentences in final summary
    alpha, beta     : weights for TextRank and LLM (alpha + beta = 1)
    sim_threshold   : redundancy threshold for cosine similarity

Output:
    S_hybrid        : final hybrid summary

Steps:

1. Merge:
    Candidates = S_textrank ∪ S_llm

2. Compare and Score:
    For each sentence s in Candidates:
        if s in S_textrank:
            TR_score = normalized TextRank score
        else:
            TR_score = 0

        if s in S_llm:
            LLM_score = semantic similarity to the original document
        else:
            LLM_score = 0

        Final_weight[s] = alpha * TR_score + beta * LLM_score

3. Rank:
    Sort Candidates by Final_weight in descending order

4. Generate Summary:
    S_hybrid = []
    For each sentence s in Candidates:
        if len(S_hybrid) < L_max AND
           similarity(s, any sentence in S_hybrid) < sim_threshold:
               Append s to S_hybrid

5. Postprocess (optional):
    Reorder S_hybrid according to original document order
    Apply minor paraphrasing to improve fluency

6. Return S_hybrid
```
---
## 5. Redundancy Handling

- Redundancy detection is performed in **embedding space**, not via exact string matching.
- This allows proper comparison between:
  - Original sentences from the document
  - Paraphrased or abstractive sentences from the LLM

---

## 6. Key Properties

### Advantages
- Combines statistical importance (TextRank) and semantic understanding (LLM).
- Prevents blind trust in LLM outputs.
- Fully explainable and algorithm-driven.
- Suitable for complexity analysis and academic evaluation.

### Limitations
- Requires sentence embeddings for similarity computation.
- Weight parameters (`alpha`, `beta`) may require tuning per dataset.
- Optional postprocessing adds minor computational cost.

---

## 7. Conclusion

The Hybrid Merge Algorithm serves as the **decision-making core** of the summarization pipeline. By explicitly controlling how extractive and abstractive information is merged, it produces summaries that are:

- Concise
- Relevant
- Semantically grounded
- Fully algorithmically transparent
