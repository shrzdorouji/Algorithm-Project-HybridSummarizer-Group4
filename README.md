# ğŸ§  Hybrid Text Summarization System

> A hybrid framework integrating graph-based extractive ranking with transformer-based abstractive modeling.

**Course:** Algorithm Design  
**Academic Year:** 1404â€“1405  

---

## ğŸ“– Overview

Automatic text summarization aims to condense a document while preserving semantic meaning and logical coherence.

This project implements a hybrid summarization framework composed of:

- TextRank â€” graph-based extractive summarization  
- Transformer-based LLM â€” abstractive language modeling  
- Hybrid fusion strategy â€” weighted integration of both approaches  

The objective is to combine algorithmic efficiency with semantic intelligence.

---

## ğŸ¯ Problem Definition

Given a document:

```
D = { s1, s2, ..., sn }
```

Generate a summary S such that:

- Relevant information is maximized  
- Redundancy is minimized  
- Logical coherence is preserved  
- Computational complexity remains bounded  

The system is analyzed both theoretically (asymptotic complexity) and empirically (runtime behavior).

---

## ğŸ— System Architecture

```
Raw Document
     â”‚
     â”œâ”€â”€ TextRank (Extractive)
     â”‚
     â”œâ”€â”€ LLM Module (Abstractive)
     â”‚
     â””â”€â”€ Hybrid Fusion
            â”‚
       Final Summary
```

---

## âš™ Algorithmic Components

### 1. TextRank (Extractive)

Pipeline:

1. Sentence segmentation  
2. Advanced preprocessing  
3. TF-IDF vectorization  
4. Sparse similarity graph construction  
5. PageRank ranking  
6. Redundancy filtering  

Complexity:

| Metric | Complexity |
|--------|------------|
| Time   | O(nÂ²L)     |
| Space  | O(nL)      |

Where:

- n = number of sentences  
- L = average sentence length  

Optimizations include sparse graph representation, KNN pruning, and early convergence stopping.

---

### 2. Transformer-Based LLM (Abstractive)

Encoder-decoder architecture with self-attention.

Complexity:

| Metric | Complexity |
|--------|------------|
| Time   | O(nÂ²)      |
| Space  | O(nÂ²)      |

The quadratic behavior originates from the attention mechanism.

---

### 3. Hybrid Fusion Strategy

Final scoring formula:

```
Score = Î± * TextRank + Î² * LLM
```

This stage performs:

- Weighted score integration  
- Similarity-based redundancy filtering  
- Logical sentence reordering  

---

## ğŸ“Š Complexity Summary

| Component | Time Complexity | Space Complexity |
|-----------|----------------|-----------------|
| TextRank  | O(nÂ²L)        | O(nL)           |
| LLM       | O(nÂ²)         | O(nÂ²)           |
| Hybrid    | O(nÂ²)         | O(n)            |

---

## ğŸ“‚ Project Structure

```
Algorithm-Project-HybridSummarizer-Group4/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ sample_texts.md
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â”‚
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ diagrams/
â”‚   â””â”€â”€ phase-1-report.md
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ textrank/
â”‚   â”‚   â”œâ”€â”€ textrank.py
â”‚   â”‚   â”œâ”€â”€ textrank_pseudocode.md
â”‚   â”‚   â””â”€â”€ complexity_analysis.md
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ llm_integration.py
â”‚   â”‚   â”œâ”€â”€ llm_role.md
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â””â”€â”€ merge/
â”‚       â”œâ”€â”€ merge_strategy.py
â”‚       â”œâ”€â”€ merge_algorithm.md
â”‚       â”œâ”€â”€ merge_algorithm_examples.md
â”‚       â”œâ”€â”€ merge_analysis.md
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸš€ Installation

```bash
git clone <repository-url>
cd Algorithm-Project-HybridSummarizer-Group4
pip install -r requirements.txt
```

---

## ğŸ§ª Example Usage

```python
from src.textrank.textrank import TextRankSummarizer

document = "Your input text here..."
summarizer = TextRankSummarizer()
summary = summarizer.summarize(document, top_k=3)
print(summary)
```

---

## ğŸ”® Future Work

- Sparse attention mechanisms  
- Sentence-BERT similarity  
- GPU acceleration  
- ROUGE-based evaluation  
- Parallel similarity computation  

---

## ğŸ‘¥ Authors

Group 4  
Algorithm Design Course  
Academic Year 1404â€“1405  
