import sys
import os
import pytest
import nltk

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…Ø§Ú˜ÙˆÙ„ textrank
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.textrank.textrank import TextRankSummarizer, sentence_segmentation


def test_summarize_deep_trace():
    """
    ØªØ³Øª Ù…Ø±Ø­Ù„Ù‡â€ŒØ¨Ù‡â€ŒÙ…Ø±Ø­Ù„Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÛŒØ§Ù†ÛŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…
    """
    # ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ (k Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ú¯Ø±Ø§Ù Ø®Ù„ÙˆØªâ€ŒØªØ± Ø¯Ø± ØªØ³Øª)
    summarizer = TextRankSummarizer(similarity_threshold=0.01, knn=2)

    document = (
        "Artificial Intelligence is a transformative technology. "
        "AI models can solve complex problems efficiently. "
        "Machine learning is a subset of artificial intelligence. "
        "The sun rises in the east every morning. "
        "Future AI systems will change how we work."
    )

    print("\n" + "=" * 50)
    print("ğŸ” Ø´Ø±ÙˆØ¹ Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ù…Ø±Ø­Ù„Ù‡â€ŒØ¨Ù‡â€ŒÙ…Ø±Ø­Ù„Ù‡ (Deep Trace)")
    print("=" * 50)

    # --- Step 1: Segmentation ---
    raw_sents = sentence_segmentation(document)
    print(f"\n[Step 1] Segmentation:")
    print(f"   - Total sentences: {len(raw_sents)}")
    for i, s in enumerate(raw_sents):
        print(f"   {i}: {s}")

    # --- Step 1.5: Advanced Preprocessing ---
    cleaned_sents = summarizer.advanced_preprocess(raw_sents)
    print(f"\n[Step 1.5] Preprocessing (Cleaned & Stemmed):")
    for i, s in enumerate(cleaned_sents):
        print(f"   {i}: {s}")

    # --- Step 2: Sentence Representation ---
    vectors = summarizer.sentence_representation(cleaned_sents)
    print(f"\n[Step 2] Representation (Sample Terms):")
    if vectors:
        print(f"   - Sent 0 keywords: {list(vectors[0].keys())}")
        print(f"   - Sent 2 keywords: {list(vectors[2].keys())}")

    # --- Step 3: Graph Construction ---
    graph = summarizer.build_similarity_graph(vectors)
    print(f"\n[Step 3] Similarity Graph (Edges):")
    for node, neighbors in graph.items():
        print(f"   - Sentence {node} connected to: {list(neighbors.keys())}")

    # --- Step 4 & 5: Ranking ---
    scores = summarizer.rank_sentences(graph)
    print(f"\n[Step 4 & 5] Final Scores (PageRank):")
    for i, score in enumerate(scores):
        bar = "â–ˆ" * int(score * 200)  # Ù†Ù…Ø§ÛŒØ´ Ø¨ØµØ±ÛŒ Ø§Ù…ØªÛŒØ§Ø²
        print(f"   Sent {i}: {score:.4f} {bar}")

    # --- Step 6 & 7: Final Summary ---
    result = summarizer.summarize(document, top_k=2)
    print(f"\n[Step 6 & 7] Final Summary (Top 2):")
    print(f"   >>> {result}")

    # Assertions
    assert len(raw_sents) == 5
    assert scores[3] < max(scores), "Ø¬Ù…Ù„Ù‡ Ù†ÙˆÛŒØ² Ù†Ø¨Ø§ÛŒØ¯ Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² Ø±Ø§ Ø¨Ú¯ÛŒØ±Ø¯"